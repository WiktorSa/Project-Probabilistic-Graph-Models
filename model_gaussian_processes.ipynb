{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv('data/preprocessed_train.csv')\n",
    "VAL = pd.read_csv('data/preprocessed_val.csv')\n",
    "TEST = pd.read_csv('data/preprocessed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return np.exp(-0.5 * sqdist / length_scale**2)\n",
    "\n",
    "\n",
    "class GaussianProcessClassifier:\n",
    "    def __init__(self, kernel=rbf_kernel, length_scale=1.0, noise=1e-6):\n",
    "        self.kernel = kernel\n",
    "        self.length_scale = length_scale\n",
    "        self.noise = noise\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        K = self.kernel(X, X, self.length_scale) + self.noise * np.eye(len(X))\n",
    "        self.L_ = cholesky(K, lower=True)\n",
    "        \n",
    "        self.alpha_ = np.zeros_like(y, dtype=float)\n",
    "        \n",
    "        def objective(alpha):\n",
    "            return 0.5 * np.dot(alpha.T, np.dot(K, alpha)) - np.sum(expit(y * alpha))\n",
    "        \n",
    "        def grad(alpha):\n",
    "            return np.dot(K, alpha) - y * expit(-y * alpha)\n",
    "        \n",
    "        result = minimize(objective, self.alpha_, jac=grad, method='L-BFGS-B')\n",
    "        self.alpha_ = result.x\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        K_trans = self.kernel(X, self.X_train, self.length_scale)\n",
    "        f_star = np.dot(K_trans, self.alpha_)\n",
    "        \n",
    "        v = solve_triangular(self.L_, K_trans.T, lower=True)\n",
    "        var_f_star = np.diag(self.kernel(X, X, self.length_scale)) - np.sum(v**2, axis=0)\n",
    "        \n",
    "        proba = expit(f_star / np.sqrt(1 + np.pi * var_f_star / 8))\n",
    "        return proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.predict_proba(X) - 0.5)\n",
    "\n",
    "\n",
    "\n",
    "class MultiClassGaussianProcessClassifier:\n",
    "    \"\"\"\n",
    "    Implements OVR classification using binary GP classifiers.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel=rbf_kernel, length_scale=1.0, noise=1e-6):\n",
    "        self.kernel = kernel\n",
    "        self.length_scale = length_scale\n",
    "        self.noise = noise\n",
    "        self.classifiers = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        for cls in self.classes_:\n",
    "            y_binary = np.where(y == cls, 1, -1)\n",
    "            gpc = GaussianProcessClassifier(kernel=self.kernel, length_scale=self.length_scale, noise=self.noise)\n",
    "            gpc.fit(X, y_binary)\n",
    "            self.classifiers[cls] = gpc\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        for idx, cls in enumerate(self.classes_):\n",
    "            proba[:, idx] = self.classifiers[cls].predict_proba(X)\n",
    "        proba = np.clip(proba, 1e-10, 1-1e-10) \n",
    "        proba /= proba.sum(axis=1, keepdims=True) \n",
    "        return proba\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = TRAIN.drop(columns='y').to_numpy()\n",
    "y_train = TRAIN.y.to_numpy()\n",
    "\n",
    "X_test = VAL.drop(columns='y').to_numpy()\n",
    "y_test = VAL.y.to_numpy()\n",
    "\n",
    "multi_gpc = MultiClassGaussianProcessClassifier(length_scale=1.0, noise=1e-6)\n",
    "multi_gpc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = multi_gpc.predict(X_test)\n",
    "\n",
    "(y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, num_runs=20):\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    aucs = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        multi_gpc = MultiClassGaussianProcessClassifier(length_scale=1.0, noise=1e-6)\n",
    "        multi_gpc.fit(X_train, y_train)\n",
    "        y_pred = multi_gpc.predict(X_test)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "        y_pred_proba = multi_gpc.predict_proba(X_test)\n",
    "        aucs.append(roc_auc_score(y_test_bin, y_pred_proba, average='weighted', multi_class='ovr'))\n",
    "    \n",
    "    print(f\"Accuracy: {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "    print(f\"F1 Score: {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "    print(f\"Precision: {np.mean(precisions):.3f} ± {np.std(precisions):.3f}\")\n",
    "    print(f\"Recall: {np.mean(recalls):.3f} ± {np.std(recalls):.3f}\")\n",
    "    print(f\"AUC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    metrics = [accuracies, f1_scores, precisions, recalls, aucs]\n",
    "    titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    \n",
    "    for i in range(5):\n",
    "        ax[i].boxplot(metrics[i])\n",
    "        ax[i].set_title(titles[i])\n",
    "        ax[i].set_xticks([1])\n",
    "        ax[i].set_xticklabels([titles[i]])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale param vs metrics (noise fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def fixed_noise(X_train, y_train, X_test, y_test, length_scales, noise=1e-6, num_runs=1):\n",
    "    results = []\n",
    "\n",
    "    for length_scale in length_scales:\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        aucs = []\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            multi_gpc = MultiClassGaussianProcessClassifier(length_scale=length_scale, noise=noise)\n",
    "            multi_gpc.fit(X_train, y_train)\n",
    "            y_pred = multi_gpc.predict(X_test)\n",
    "            \n",
    "            accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "            recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "            \n",
    "            y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "            y_pred_proba = multi_gpc.predict_proba(X_test)\n",
    "            aucs.append(roc_auc_score(y_test_bin, y_pred_proba, average='weighted', multi_class='ovr'))\n",
    "        \n",
    "        results.append({\n",
    "            'length_scale': length_scale,\n",
    "            'accuracy_mean': np.mean(accuracies),\n",
    "            'accuracy_std': np.std(accuracies),\n",
    "            'f1_mean': np.mean(f1_scores),\n",
    "            'f1_std': np.std(f1_scores),\n",
    "            'precision_mean': np.mean(precisions),\n",
    "            'precision_std': np.std(precisions),\n",
    "            'recall_mean': np.mean(recalls),\n",
    "            'recall_std': np.std(recalls),\n",
    "            'auc_mean': np.mean(aucs),\n",
    "            'auc_std': np.std(aucs)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Plotting function\n",
    "def plot_results(results):\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    metrics = ['accuracy', 'f1', 'precision', 'recall', 'auc']\n",
    "    titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        metric_mean = [res[f'{metric}_mean'] for res in results]\n",
    "        metric_std = [res[f'{metric}_std'] for res in results]\n",
    "        \n",
    "        ax[i // 3, i % 3].plot(range(len(results)), metric_mean, marker='o', label=f'{titles[i]} Mean')\n",
    "        ax[i // 3, i % 3].fill_between(range(len(results)), \n",
    "                                        np.array(metric_mean) - np.array(metric_std), \n",
    "                                        np.array(metric_mean) + np.array(metric_std), \n",
    "                                        alpha=0.2, label=f'{titles[i]} Std')\n",
    "        \n",
    "        ax[i // 3, i % 3].set_title(titles[i])\n",
    "        ax[i // 3, i % 3].set_xticks(range(len(results)))\n",
    "        ax[i // 3, i % 3].set_xticklabels([f\"LS: {res['length_scale']}\" for res in results], rotation=45, ha=\"right\")\n",
    "        ax[i // 3, i % 3].legend()\n",
    "    ax[1,2].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length_scales = [0.1, 0.2, 0.5, 1.0, 5.0, 10.0]\n",
    "fixed_noise_val = 1e-6\n",
    "\n",
    "results = fixed_noise(X_train, y_train, X_test, y_test, length_scales, noise=fixed_noise_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise param vs metrics (scale fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_scale(X_train, y_train, X_test, y_test, noises, length_scale=1.0, num_runs=1):\n",
    "    results = []\n",
    "\n",
    "    for noise in noises:\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        aucs = []\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            multi_gpc = MultiClassGaussianProcessClassifier(length_scale=length_scale, noise=noise)\n",
    "            multi_gpc.fit(X_train, y_train)\n",
    "            y_pred = multi_gpc.predict(X_test)\n",
    "            \n",
    "            accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "            recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "            \n",
    "            y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "            y_pred_proba = multi_gpc.predict_proba(X_test)\n",
    "            aucs.append(roc_auc_score(y_test_bin, y_pred_proba, average='weighted', multi_class='ovr'))\n",
    "        \n",
    "        results.append({\n",
    "            'noise': noise,\n",
    "            'accuracy_mean': np.mean(accuracies),\n",
    "            'accuracy_std': np.std(accuracies),\n",
    "            'f1_mean': np.mean(f1_scores),\n",
    "            'f1_std': np.std(f1_scores),\n",
    "            'precision_mean': np.mean(precisions),\n",
    "            'precision_std': np.std(precisions),\n",
    "            'recall_mean': np.mean(recalls),\n",
    "            'recall_std': np.std(recalls),\n",
    "            'auc_mean': np.mean(aucs),\n",
    "            'auc_std': np.std(aucs)\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 1.0\n",
    "noises = [1e-3,2e-3,3e-3, 4e-3,5e-3,6e-3,7e-3,8e-3, 1e-4, 5e-4,1e-5, 5e-5, 1e-6]\n",
    "results = fixed_scale(X_train, y_train, X_test, y_test, noises, length_scale=length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_nresults(results):\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    metrics = ['accuracy', 'f1', 'precision', 'recall', 'auc']\n",
    "    titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        metric_mean = [res[f'{metric}_mean'] for res in results]\n",
    "        metric_std = [res[f'{metric}_std'] for res in results]\n",
    "        \n",
    "        ax[i // 3, i % 3].plot(range(len(results)), metric_mean, marker='o', label=f'{titles[i]} Mean')\n",
    "        ax[i // 3, i % 3].fill_between(range(len(results)), \n",
    "                                        np.array(metric_mean) - np.array(metric_std), \n",
    "                                        np.array(metric_mean) + np.array(metric_std), \n",
    "                                        alpha=0.2, label=f'{titles[i]} Std')\n",
    "        \n",
    "        ax[i // 3, i % 3].set_title(titles[i])\n",
    "        ax[i // 3, i % 3].set_xticks(range(len(results)))\n",
    "        ax[i // 3, i % 3].set_xticklabels([f\"noise: {res['noise']}\" for res in results], rotation=45, ha=\"right\")\n",
    "        ax[i // 3, i % 3].legend()\n",
    "    ax[1,2].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_nresults(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  f1_score\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'length_scale': trial.suggest_float('length_scale', 0.1, 15),\n",
    "        'noise': trial.suggest_float('noise', 1e-6, 1e-3)\n",
    "    }\n",
    "    multi_gpc = MultiClassGaussianProcessClassifier(length_scale=param['length_scale'], noise=param['noise'])\n",
    "    multi_gpc.fit(X_train, y_train)\n",
    "    y_pred = multi_gpc.predict(X_test) # to walidacyjny tak naprawde\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "best_trial = study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use only interactively, will be empty in Jupyter Lab due to optuna errors\n",
    "def visualize_optuna(study):\n",
    "    import optuna.visualization as vis\n",
    "    \n",
    "    optuna.visualization.plot_optimization_history(study).show()\n",
    "    \n",
    "    optuna.visualization.plot_param_importances(study).show()\n",
    "    \n",
    "    optuna.visualization.plot_slice(study).show()\n",
    "    \n",
    "    optuna.visualization.plot_parallel_coordinate(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optuna(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = TRAIN.drop(columns='y').to_numpy()\n",
    "y_train = TRAIN.y.to_numpy()\n",
    "X_val = VAL.drop(columns='y').to_numpy()\n",
    "y_val = VAL.y.to_numpy()\n",
    "\n",
    "X_test = TEST.drop(columns='y').to_numpy()\n",
    "y_test = TEST.y.to_numpy()\n",
    "\n",
    "X_train_combined = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_val), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def evaluate_final_model(X_train, y_train, X_test, y_test, num_runs=20):\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    aucs = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        multi_gpc = MultiClassGaussianProcessClassifier(length_scale=0.9370589034485247, noise=0.0007533926368261847)\n",
    "        multi_gpc.fit(X_train, y_train)\n",
    "        y_pred = multi_gpc.predict(X_test)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "        y_pred_proba = multi_gpc.predict_proba(X_test)\n",
    "        aucs.append(roc_auc_score(y_test_bin, y_pred_proba, average='weighted', multi_class='ovr'))\n",
    "    \n",
    "    print(f\"Accuracy: {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "    print(f\"F1 Score: {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "    print(f\"Precision: {np.mean(precisions):.3f} ± {np.std(precisions):.3f}\")\n",
    "    print(f\"Recall: {np.mean(recalls):.3f} ± {np.std(recalls):.3f}\")\n",
    "    print(f\"AUC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    metrics = [accuracies, f1_scores, precisions, recalls, aucs]\n",
    "    titles = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    \n",
    "    for i in range(5):\n",
    "        ax[i].boxplot(metrics[i])\n",
    "        ax[i].set_title(titles[i])\n",
    "        ax[i].set_xticks([1])\n",
    "        ax[i].set_xticklabels([titles[i]])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_final_model(X_train_combined, y_train_combined, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
