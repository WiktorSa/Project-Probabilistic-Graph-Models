{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, CategoricalNB\n",
    "from sklearn import metrics as sklearn_metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN: pd.DataFrame = None\n",
    "VAL: pd.DataFrame = None\n",
    "TEST: pd.DataFrame = None\n",
    "\n",
    "def show_metrics(predict: Callable[[pd.DataFrame], pd.DataFrame], show_only_test: bool = False):\n",
    "    for split, name in zip([TRAIN, VAL, TEST], ['train', 'val', 'test']):\n",
    "        y_true = split['y']\n",
    "        y_pred = predict(split.drop('y', axis=1))\n",
    "        y_true_binarized = label_binarize(y_true, classes=range(6))\n",
    "        y_pred_binarized = label_binarize(y_pred, classes=range(6))\n",
    "        res = {\n",
    "            'accuracy': sklearn_metrics.accuracy_score(y_true, y_pred),\n",
    "            'precision': sklearn_metrics.precision_score(y_true, y_pred, average='weighted'),\n",
    "            'recall': sklearn_metrics.recall_score(y_true, y_pred, average='weighted'),\n",
    "            'f1': sklearn_metrics.f1_score(y_true, y_pred, average='weighted'),\n",
    "            'auc': sklearn_metrics.roc_auc_score(y_true_binarized, y_pred_binarized, average='weighted', multi_class='ovr'),\n",
    "        }\n",
    "        res = pd.DataFrame([res])\n",
    "        # if not show_only_test: display(res)\n",
    "        # elif name == 'test': display(res)\n",
    "        if name == 'test': return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = None\n",
    "for plabel, pkwargs in [\n",
    "    ('one hot', dict(categorical_to_numerical_scale=False, categorical_to_one_hot=True, continous_to_discrete=False)),\n",
    "    ('ordinal', dict(categorical_to_numerical_scale=True, categorical_to_one_hot=False, continous_to_discrete=False)),\n",
    "    ('ordinal 20 bins', dict(categorical_to_numerical_scale=True, categorical_to_one_hot=False, continous_to_discrete=True)),\n",
    "]:\n",
    "    preprocess(**pkwargs)\n",
    "    TRAIN = pd.read_csv('data/preprocessed_train.csv')\n",
    "    VAL = pd.read_csv('data/preprocessed_val.csv')\n",
    "    TEST = pd.read_csv('data/preprocessed_test.csv')\n",
    "    for mlabel, model in [\n",
    "        ('GaussianNB', GaussianNB(priors=None)), # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "        ('MultinomialNB', MultinomialNB(alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None)), # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "        ('BernoulliNB', BernoulliNB(alpha=1.0, force_alpha=True, binarize=0.0, fit_prior=True, class_prior=None)), # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "        ('CategoricalNB', CategoricalNB(alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None, min_categories=None)), # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB\n",
    "    ]:\n",
    "        model.fit(X=TRAIN.drop('y', axis=1), y=TRAIN['y'])\n",
    "        res = show_metrics(lambda x: model.predict(x), show_only_test=True)\n",
    "        res['preprocessing'] = plabel\n",
    "        res['model'] = mlabel\n",
    "        if metrics is None: metrics = res\n",
    "        else: metrics = pd.concat([metrics, res], ignore_index=True)\n",
    "metrics = metrics[['preprocessing','model','accuracy','precision','recall','f1','auc']]\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv('data/preprocessed_train.csv')\n",
    "VAL = pd.read_csv('data/preprocessed_val.csv')\n",
    "TEST = pd.read_csv('data/preprocessed_test.csv')\n",
    "\n",
    "col_cat = ['Gender','Age','family_history_with_overweight','FAVC','FCVC','NCP','CAEC','SMOKE','CH2O','SCC','FAF','TUE','CALC','MTRANS']\n",
    "# col_cat = ['Gender','family_history_with_overweight','FAVC','CAEC','SCC','CALC','SMOKE','MTRANS']\n",
    "# col_cat = []\n",
    "\n",
    "for c in col_cat: TRAIN[c] = TRAIN[c].astype('category')\n",
    "for c in col_cat: VAL[c] = VAL[c].astype('category')\n",
    "for c in col_cat: TEST[c] = TEST[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNBClassifier:\n",
    "    def __init__(self, num_epochs=500, lr=1e-2, optimizer=None, loss=None):\n",
    "        self._num_epochs = num_epochs\n",
    "        self._lr = lr\n",
    "        self._optimizer = optimizer if optimizer is not None else pyro.optim.Adam({'lr': self._lr})\n",
    "        self._loss = loss if loss is not None else pyro.infer.Trace_ELBO()\n",
    "        self._num_cls = None\n",
    "        self._c_logits = None\n",
    "        self._num_probs = None\n",
    "        self._cat_probs = None\n",
    "        \n",
    "    def fit(self, X, y, valX=None, valy=None):\n",
    "        pyro.clear_param_store()\n",
    "        categorical_cols = X.select_dtypes('category').columns.values\n",
    "        numerical_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "        print('num', numerical_cols)\n",
    "        print('cat', categorical_cols)\n",
    "        svi = pyro.infer.SVI(model=self._model, guide=self._guide,\n",
    "            optim=self._optimizer, loss=self._loss)\n",
    "        accs = []\n",
    "        losses = []\n",
    "        val_accs = []\n",
    "        val_losses = []\n",
    "        for i in tqdm(range(self._num_epochs)):\n",
    "            losses.append(svi.step(X, y))\n",
    "            if valX is not None: val_losses.append(self._loss.loss(self._model, self._guide, valX, valy))\n",
    "            if (i+1) % 10 == 0:\n",
    "                accs.append(sklearn_metrics.accuracy_score(y, self.predict(X)))\n",
    "                if valX is not None: val_accs.append(sklearn_metrics.accuracy_score(valy, self.predict(valX)))\n",
    "        return accs, losses, val_accs, val_losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = pyro.infer.Predictive(model=self._model, guide=self._guide,\n",
    "            num_samples=1, return_sites=('logP(c|x)',))\n",
    "        log_pcx = pred(X)['logP(c|x)'].detach().squeeze(0).squeeze(0)\n",
    "        y_pred = torch.argmax(log_pcx, dim=-1)\n",
    "        return y_pred\n",
    "    \n",
    "    def _model(self, X, y=None):    \n",
    "        if y is None: # inference mode\n",
    "            self._get_classes_log_probs(X)\n",
    "            return\n",
    "        self._num_cls = max(y) + 1\n",
    "        categorical_cols = X.select_dtypes('category').columns.values\n",
    "        numerical_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "        self._init_c_logits()\n",
    "        self._init_num_params(X, numerical_cols)\n",
    "        self._init_cat_params(X, categorical_cols)\n",
    "        self._observe_numerical_features_given_classes(X, y)\n",
    "        self._observe_categorical_features_given_classes(X, y)\n",
    "        \n",
    "    def _guide(self, X, y=None):\n",
    "        pass  # This is meant to be an empty function\n",
    "    \n",
    "    def _init_c_logits(self):\n",
    "        self._c_probs = pyro.param('c_probs',\n",
    "            lambda: torch.ones(self._num_cls).div(self._num_cls),\n",
    "            constraint=torch.distributions.constraints.simplex)\n",
    "        \n",
    "    def _init_num_params(self, X, numerical_cols):\n",
    "        self._num_dists = {\n",
    "            col: {\n",
    "                'mu': pyro.param(f'{col}_mu', lambda: torch.zeros(self._num_cls)),\n",
    "                'sigma': pyro.param(f'{col}_sigma', lambda: torch.ones(self._num_cls),\n",
    "                    constraint=torch.distributions.constraints.positive),\n",
    "            } for col in numerical_cols\n",
    "        }\n",
    "    \n",
    "    def _init_cat_params(self, X, categorical_cols):  # Add\n",
    "        self._cat_logits = {\n",
    "            col: pyro.param(f'{col}_logits', lambda: torch.ones([self._num_cls, len(X[col].cat.categories)]))\n",
    "            for col in categorical_cols\n",
    "        }\n",
    "        \n",
    "    def _observe_numerical_features_given_classes(self, X, y):\n",
    "        for c in range(self._num_cls):\n",
    "            x_c = X[y==c]\n",
    "            with pyro.plate(f'data-numerical-{c}', x_c.shape[0]):\n",
    "                for nc, v in self._num_dists.items():\n",
    "                    pyro.sample(f'x_{nc}|c={c}', \n",
    "                        pyro.distributions.Normal(v['mu'][c], v['sigma'][c]),\n",
    "                        obs=torch.tensor(x_c[nc].values))\n",
    "\n",
    "    def _observe_categorical_features_given_classes(self, X, y):  # Add\n",
    "        for c in range(self._num_cls):\n",
    "            x_c = X[y==c]\n",
    "            with pyro.plate(f'data-categorical-{c}', x_c.shape[0]):\n",
    "                for cc, v in self._cat_logits.items():\n",
    "                    pyro.sample(f'x_{cc}|c={c}',\n",
    "                        pyro.distributions.Categorical(logits=v[c]),\n",
    "                        obs=torch.tensor(x_c[cc].values))\n",
    "\n",
    "    def _get_log_likelihood(self, X):\n",
    "        log_lk = []\n",
    "        for c in range(self._num_cls):\n",
    "            lps = []\n",
    "            lps.extend([\n",
    "                pyro.distributions.Normal(v['mu'][c], v['sigma'][c]).log_prob(torch.tensor(X[nc].values))\n",
    "                for nc, v in self._num_dists.items()])\n",
    "            lps.extend([\n",
    "                pyro.distributions.Categorical(logits=v[c]).log_prob(torch.tensor(X[cc].values))\n",
    "                for cc, v in self._cat_logits.items()])\n",
    "            log_lk.append(torch.stack(lps).sum(dim=0))\n",
    "        return torch.stack(log_lk).t()\n",
    "    \n",
    "    def _get_classes_log_probs(self, X):\n",
    "        log_lk = self._get_log_likelihood(X)\n",
    "        log_pcx = pyro.deterministic('logP(c|x)', self._c_probs.log() + log_lk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNBClassifier(num_epochs=1000)\n",
    "model.fit(X=TRAIN.drop('y', axis=1), y=TRAIN['y'])\n",
    "show_metrics(lambda x: model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = None\n",
    "for lr in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    model = GaussianNBClassifier(num_epochs=1000, lr=lr)\n",
    "    model.fit(X=TRAIN.drop('y', axis=1), y=TRAIN['y'])\n",
    "    res = show_metrics(lambda x: model.predict(x))\n",
    "    res['lr'] = lr\n",
    "    if metrics is None: metrics = res\n",
    "    else: metrics = pd.concat([metrics, res], ignore_index=True)\n",
    "\n",
    "metrics = metrics[['lr','accuracy','precision','recall','f1','auc']]\n",
    "display(metrics)\n",
    "# display(metrics.to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight decay experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = None\n",
    "for wd in [1e-3, 1e-4]:\n",
    "    model = GaussianNBClassifier(num_epochs=1000, lr=1e-1, optimizer=pyro.optim.Adam({'lr': 1e-1, 'weight_decay': wd}))\n",
    "    model.fit(X=TRAIN.drop('y', axis=1), y=TRAIN['y'])\n",
    "    res = show_metrics(lambda x: model.predict(x))\n",
    "    res['wd'] = wd\n",
    "    if metrics is None: metrics = res\n",
    "    else: metrics = pd.concat([metrics, res], ignore_index=True)\n",
    "\n",
    "metrics = metrics[['wd','accuracy','precision','recall','f1','auc']]\n",
    "display(metrics)\n",
    "# display(metrics.to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss and acc graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNBClassifier(num_epochs=1000)\n",
    "accs, losses, val_accs, val_losses = model.fit(X=TRAIN.drop('y', axis=1), y=TRAIN['y'], valX=VAL.drop('y', axis=1), valy=VAL['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(len(losses)), losses, label='Train', color='b')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='Validation', color='r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot([i*10 for i in range(len(accs))], accs, label='Train', color='b')\n",
    "plt.plot([i*10 for i in range(len(val_accs))], val_accs, label='Validation', color='r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
